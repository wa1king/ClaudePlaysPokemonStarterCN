#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIæ€§èƒ½æµ‹è¯•å·¥å…·
ç”¨äºæµ‹è¯•ä¸åŒClaudeæ¨¡å‹çš„å“åº”æ—¶é—´å’Œæ€§èƒ½è¡¨ç°
"""

import time
import json
import base64
import io
from datetime import datetime
from anthropic import Anthropic
from PIL import Image, ImageDraw

# æµ‹è¯•é…ç½®
TEST_CONFIG = {
    "api_key": "sk-ksbCidPFhLiGTRo_I4fflYASF92UHYC8S1DNHp2kkbTaJkRShC8oOKPIJdI",
    "base_url": "https://chatapi.leyidc.net/claude",
    "models": [
        "claude-3-haiku-20240307",           # æœ€å¿«çš„æ¨¡å‹
        "claude-3-5-haiku-20241022",         # æ–°ç‰ˆHaiku
        "claude-3-5-sonnet-20241022",        # å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        "claude-3-sonnet-20240229",          # æ—§ç‰ˆSonnet
    ],
    "max_tokens_options": [512, 1024, 2048],  # ä¸åŒçš„tokené™åˆ¶
    "test_rounds": 3,  # æ¯ä¸ªé…ç½®æµ‹è¯•è½®æ•°
}

def create_test_screenshot():
    """åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æ¸¸æˆæˆªå›¾ç”¨äºæµ‹è¯•"""
    # åˆ›å»ºä¸€ä¸ª160x144çš„å›¾åƒï¼ˆGame Boyåˆ†è¾¨ç‡ï¼‰
    img = Image.new('RGB', (160, 144), color='lightgreen')
    draw = ImageDraw.Draw(img)
    
    # ç»˜åˆ¶ä¸€äº›ç®€å•çš„æ¸¸æˆå…ƒç´ 
    # ä¸»è§’
    draw.rectangle([75, 65, 85, 85], fill='red', outline='black')
    
    # ä¸€äº›å»ºç­‘ç‰©
    draw.rectangle([20, 20, 50, 50], fill='brown', outline='black')
    draw.rectangle([110, 30, 140, 60], fill='blue', outline='black')
    
    # é“è·¯
    draw.rectangle([0, 90, 160, 110], fill='gray')
    
    # æ ‘æœ¨
    for x in [30, 60, 100, 130]:
        draw.ellipse([x-5, 70, x+5, 80], fill='green')
    
    # è½¬æ¢ä¸ºbase64
    buffered = io.BytesIO()
    img.save(buffered, format="PNG")
    return base64.standard_b64encode(buffered.getvalue()).decode()

# æµ‹è¯•ç”¨çš„ç³»ç»Ÿæç¤º
SYSTEM_PROMPT = """ä½ æ­£åœ¨ç©å®å¯æ¢¦çº¢ç‰ˆã€‚ä½ å¯ä»¥çœ‹åˆ°æ¸¸æˆç”»é¢å¹¶é€šè¿‡æ‰§è¡Œæ¨¡æ‹Ÿå™¨å‘½ä»¤æ¥æ§åˆ¶æ¸¸æˆã€‚

ä½ çš„ç›®æ ‡æ˜¯é€šå…³å®å¯æ¢¦çº¢ç‰ˆå¹¶æœ€ç»ˆå‡»è´¥å››å¤©ç‹ã€‚æ ¹æ®ä½ åœ¨å±å¹•ä¸Šçœ‹åˆ°çš„å†…å®¹åšå‡ºå†³ç­–ã€‚

åœ¨æ¯ä¸ªåŠ¨ä½œä¹‹å‰ï¼Œç®€è¦è§£é‡Šä½ çš„æ¨ç†ï¼Œç„¶åé€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚"""

# æµ‹è¯•ç”¨çš„å·¥å…·å®šä¹‰
AVAILABLE_TOOLS = [
    {
        "name": "press_buttons",
        "description": "Press a sequence of buttons on the Game Boy.",
        "input_schema": {
            "type": "object",
            "properties": {
                "buttons": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "enum": ["a", "b", "start", "select", "up", "down", "left", "right"]
                    },
                    "description": "List of buttons to press in sequence."
                }
            },
            "required": ["buttons"],
        },
    }
]

def create_test_messages():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„æ¶ˆæ¯å†å²"""
    screenshot_b64 = create_test_screenshot()
    
    return [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä½ ç°åœ¨å¯ä»¥å¼€å§‹æ¸¸æˆäº†ã€‚è¿™æ˜¯å½“å‰çš„æ¸¸æˆæˆªå›¾ï¼š"
                },
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": screenshot_b64,
                    },
                },
                {
                    "type": "text",
                    "text": "æ¸¸æˆçŠ¶æ€ä¿¡æ¯:\nä½ç½®: çœŸæ–°é•‡\nä¸»è§’: å°æ™º\nå®å¯æ¢¦é˜Ÿä¼: çš®å¡ä¸˜ (Lv.5)\n\nè¯·é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚"
                }
            ]
        }
    ]

def test_api_call(client, model, max_tokens, messages, tools):
    """æ‰§è¡Œå•æ¬¡APIè°ƒç”¨æµ‹è¯•"""
    start_time = time.time()
    
    try:
        response = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            system=SYSTEM_PROMPT,
            messages=messages,
            tools=tools,
            temperature=0.7,
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        # æå–å“åº”ä¿¡æ¯
        response_text = ""
        tool_calls = []
        
        for block in response.content:
            if block.type == "text":
                response_text += block.text
            elif block.type == "tool_use":
                tool_calls.append(block.name)
        
        return {
            "success": True,
            "duration": duration,
            "usage": response.usage,
            "response_length": len(response_text),
            "tool_calls": tool_calls,
            "error": None
        }
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        return {
            "success": False,
            "duration": duration,
            "usage": None,
            "response_length": 0,
            "tool_calls": [],
            "error": str(e)
        }

def run_performance_test():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹APIæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = Anthropic(
        api_key=TEST_CONFIG["api_key"],
        base_url=TEST_CONFIG["base_url"]
    )
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    messages = create_test_messages()
    tools = AVAILABLE_TOOLS
    
    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = []
    
    # éå†æ‰€æœ‰é…ç½®ç»„åˆ
    for model in TEST_CONFIG["models"]:
        for max_tokens in TEST_CONFIG["max_tokens_options"]:
            print(f"\nğŸ“Š æµ‹è¯•é…ç½®: {model} (max_tokens={max_tokens})")
            print("-" * 50)
            
            model_results = []
            
            # å¤šè½®æµ‹è¯•
            for round_num in range(TEST_CONFIG["test_rounds"]):
                print(f"   ç¬¬{round_num + 1}è½®æµ‹è¯•...", end=" ")
                
                result = test_api_call(client, model, max_tokens, messages, tools)
                model_results.append(result)
                
                if result["success"]:
                    print(f"âœ… {result['duration']:.3f}ç§’")
                else:
                    print(f"âŒ å¤±è´¥: {result['error']}")
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            successful_results = [r for r in model_results if r["success"]]
            
            if successful_results:
                durations = [r["duration"] for r in successful_results]
                avg_duration = sum(durations) / len(durations)
                min_duration = min(durations)
                max_duration = max(durations)
                
                # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
                if successful_results[0]["usage"]:
                    avg_input_tokens = sum(r["usage"].input_tokens for r in successful_results) / len(successful_results)
                    avg_output_tokens = sum(r["usage"].output_tokens for r in successful_results) / len(successful_results)
                else:
                    avg_input_tokens = 0
                    avg_output_tokens = 0
                
                print(f"   ğŸ“ˆ å¹³å‡è€—æ—¶: {avg_duration:.3f}ç§’")
                print(f"   âš¡ æœ€å¿«: {min_duration:.3f}ç§’")
                print(f"   ğŸŒ æœ€æ…¢: {max_duration:.3f}ç§’")
                print(f"   ğŸ“ å¹³å‡è¾“å…¥tokens: {avg_input_tokens:.0f}")
                print(f"   ğŸ“¤ å¹³å‡è¾“å‡ºtokens: {avg_output_tokens:.0f}")
                print(f"   âœ… æˆåŠŸç‡: {len(successful_results)}/{len(model_results)}")
                
                # ä¿å­˜ç»“æœ
                all_results.append({
                    "model": model,
                    "max_tokens": max_tokens,
                    "avg_duration": avg_duration,
                    "min_duration": min_duration,
                    "max_duration": max_duration,
                    "success_rate": len(successful_results) / len(model_results),
                    "avg_input_tokens": avg_input_tokens,
                    "avg_output_tokens": avg_output_tokens,
                    "raw_results": model_results
                })
            else:
                print(f"   âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    generate_performance_report(all_results)
    
    return all_results

def generate_performance_report(results):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("=" * 60)
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
        return
    
    # æŒ‰å¹³å‡è€—æ—¶æ’åº
    results.sort(key=lambda x: x["avg_duration"])
    
    print("\nğŸ† æ€§èƒ½æ’è¡Œæ¦œ (æŒ‰å¹³å‡å“åº”æ—¶é—´æ’åº):")
    print("-" * 60)
    
    for i, result in enumerate(results, 1):
        model_short = result["model"].split("-")[-1] if "-" in result["model"] else result["model"]
        print(f"{i:2d}. {result['model']:<35} "
              f"({result['max_tokens']:4d} tokens) "
              f"{result['avg_duration']:6.3f}ç§’")
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_result = results[0]
    print(f"\nğŸ¥‡ æœ€ä½³é…ç½®:")
    print(f"   æ¨¡å‹: {best_result['model']}")
    print(f"   Max Tokens: {best_result['max_tokens']}")
    print(f"   å¹³å‡è€—æ—¶: {best_result['avg_duration']:.3f}ç§’")
    print(f"   æˆåŠŸç‡: {best_result['success_rate']*100:.1f}%")
    
    # é€Ÿåº¦å¯¹æ¯”
    if len(results) > 1:
        slowest_result = results[-1]
        speedup = slowest_result["avg_duration"] / best_result["avg_duration"]
        print(f"   ç›¸æ¯”æœ€æ…¢é…ç½®å¿« {speedup:.1f}x")
    
    # Tokenæ•ˆç‡åˆ†æ
    print(f"\nğŸ“Š Tokenä½¿ç”¨æ•ˆç‡:")
    print("-" * 40)
    for result in results[:5]:  # åªæ˜¾ç¤ºå‰5å
        if result["avg_input_tokens"] > 0:
            tokens_per_second = (result["avg_input_tokens"] + result["avg_output_tokens"]) / result["avg_duration"]
            model_short = result["model"].split("-")[-1] if "-" in result["model"] else result["model"]
            print(f"{model_short:15s}: {tokens_per_second:6.0f} tokens/ç§’")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"api_performance_report_{timestamp}.json"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def quick_test(model_name, max_tokens=1024):
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"ğŸ” å¿«é€Ÿæµ‹è¯•: {model_name} (max_tokens={max_tokens})")
    
    client = Anthropic(
        api_key=TEST_CONFIG["api_key"],
        base_url=TEST_CONFIG["base_url"]
    )
    
    messages = create_test_messages()
    result = test_api_call(client, model_name, max_tokens, messages, AVAILABLE_TOOLS)
    
    if result["success"]:
        print(f"âœ… æˆåŠŸ! è€—æ—¶: {result['duration']:.3f}ç§’")
        if result["usage"]:
            print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥{result['usage'].input_tokens}, è¾“å‡º{result['usage'].output_tokens}")
        print(f"ğŸ“ å“åº”é•¿åº¦: {result['response_length']} å­—ç¬¦")
        if result["tool_calls"]:
            print(f"ğŸ› ï¸ å·¥å…·è°ƒç”¨: {', '.join(result['tool_calls'])}")
    else:
        print(f"âŒ å¤±è´¥: {result['error']}")
    
    return result

if __name__ == "__main__":
    print("ğŸ® Claude APIæ€§èƒ½æµ‹è¯•å·¥å…·")
    print("=" * 40)
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å®Œæ•´æ€§èƒ½æµ‹è¯• (æµ‹è¯•æ‰€æœ‰æ¨¡å‹å’Œé…ç½®)")
    print("2. å¿«é€Ÿæµ‹è¯• (æµ‹è¯•å•ä¸ªæ¨¡å‹)")
    print("3. æ¨èé…ç½®æµ‹è¯• (åªæµ‹è¯•æ¨èçš„å‡ ä¸ªé…ç½®)")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            # å®Œæ•´æµ‹è¯•
            results = run_performance_test()
            
        elif choice == "2":
            # å¿«é€Ÿæµ‹è¯•
            print("\nå¯ç”¨æ¨¡å‹:")
            for i, model in enumerate(TEST_CONFIG["models"], 1):
                print(f"{i}. {model}")
            
            model_choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(TEST_CONFIG['models'])}): ").strip()
            try:
                model_index = int(model_choice) - 1
                if 0 <= model_index < len(TEST_CONFIG["models"]):
                    model = TEST_CONFIG["models"][model_index]
                    quick_test(model)
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æ•°å­—")
                
        elif choice == "3":
            # æ¨èé…ç½®æµ‹è¯•
            recommended_configs = [
                ("claude-3-haiku-20240307", 1024),
                ("claude-3-5-haiku-20241022", 1024),
                ("claude-3-5-sonnet-20241022", 1024),
            ]
            
            print("\nğŸ¯ æµ‹è¯•æ¨èé…ç½®...")
            for model, max_tokens in recommended_configs:
                print(f"\næµ‹è¯• {model}...")
                quick_test(model, max_tokens)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    print("\nğŸ æµ‹è¯•å®Œæˆ!")